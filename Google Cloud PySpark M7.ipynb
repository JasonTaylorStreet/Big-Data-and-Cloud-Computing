{"cells": [{"cell_type": "code", "execution_count": 1, "id": "56c23452-5330-48a9-bc5e-f16b54f7547f", "metadata": {}, "outputs": [], "source": "#0: Start a Spark Session and import the libraries you will need.\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder.getOrCreate()"}, {"cell_type": "code", "execution_count": 2, "id": "a8d34642-ddfe-4e83-a495-164a7c7b39b4", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Read the stock data from the GCP bucket and save it as a data frame.\namzn_df = spark.read.csv(\"gs://jpstreet-5132/assignment/AMZN.csv\", header=True, inferSchema=True)"}, {"cell_type": "code", "execution_count": 3, "id": "339eb4e8-bf8d-49da-94ab-66bbe5d995fb", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Date: string (nullable = true)\n |-- Open: double (nullable = true)\n |-- High: double (nullable = true)\n |-- Low: double (nullable = true)\n |-- Close: double (nullable = true)\n |-- Adj Close: double (nullable = true)\n |-- Volume: integer (nullable = true)\n\n"}], "source": "#1: Display the Schema of the data frame.\namzn_df.printSchema()"}, {"cell_type": "code", "execution_count": 4, "id": "68154123-c011-42db-83c1-81b9e07c5314", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+----------+-----------------+-----------------+-----------------+------------------+------------------+-------------------+\n|summary|      Date|             Open|             High|              Low|             Close|         Adj Close|             Volume|\n+-------+----------+-----------------+-----------------+-----------------+------------------+------------------+-------------------+\n|  count|      2518|             2518|             2518|             2518|              2518|              2518|               2518|\n|   mean|      null|73.82353615726771|74.66189798570285| 72.8813564964257| 73.78004878752976| 73.78004878752976|8.025619868943606E7|\n| stddev|      null|53.34565607175615|53.99876250199932|52.61413456378813|53.289557886169824|53.289557886169824|4.230300092993142E7|\n|    min|2013-01-02|           12.447|          12.6465|          12.2875|           12.4115|           12.4115|           17626000|\n|    max|2022-12-30|       187.199997|       188.654007|       184.839493|        186.570496|        186.570496|          477122000|\n+-------+----------+-----------------+-----------------+-----------------+------------------+------------------+-------------------+\n\n"}], "source": "#2: Display the summary statistics for all columns in the data frame.\namzn_df.describe().show()"}, {"cell_type": "code", "execution_count": 5, "id": "f546d516-5701-4103-a644-4c682376c981", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 5:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+----------+----------+----------+----------+----------+---------+\n|      Date|      Open|      High|       Low|     Close| Adj Close|   Volume|\n+----------+----------+----------+----------+----------+----------+---------+\n|2021-07-13|185.104996|188.654007|183.565994|183.867996|183.867996| 76918000|\n|2021-11-19|185.634506|188.107498|183.785995|183.828506|183.828506| 98734000|\n|2021-07-08|182.177994|187.999496|   181.056|186.570496|186.570496|103612000|\n|2021-07-12|187.199997|187.864502|184.839493|185.927505|185.927505| 51432000|\n|2021-07-09|186.126007|187.399994|184.669998|185.966995|185.966995| 74964000|\n+----------+----------+----------+----------+----------+----------+---------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "#3: Find and display the five records that has the highest price in the High column.\namzn_df.orderBy(amzn_df[\"High\"].desc()).show(5)"}, {"cell_type": "code", "execution_count": 6, "id": "16347382-6047-4a85-b220-bc22dcc941cd", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-------+\n|      Date|    Low|\n+----------+-------+\n|2013-05-01|12.2875|\n+----------+-------+\nonly showing top 1 row\n\n"}], "source": "#4: What day had the lowest price? Display the date (as given in the Date column) and the price?\namzn_df.orderBy(amzn_df[\"Low\"]).select(\"Date\", \"Low\").show(1)"}, {"cell_type": "code", "execution_count": 7, "id": "02258c11-d2f6-4c3b-89ad-947afe728e0b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Range of Volume: 459496000\n"}], "source": "#5: What is the range of the Volume column? (Hint: Range is the difference between max and min values)\nvolume_range = amzn_df.agg({\"Volume\": \"max\"}).first()[0] - amzn_df.agg({\"Volume\": \"min\"}).first()[0]\nprint(f\"Range of Volume: {volume_range}\")"}, {"cell_type": "code", "execution_count": 8, "id": "a3e47995-ff14-4f63-871f-e6272fe9b42f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Average High: 74.66189798570285, Percent greater than average: 47.97%\n"}], "source": "#6: Calculate the average of the High column. What percent of the observations had a High price greater than the average?\navg_high = amzn_df.agg({\"High\": \"avg\"}).first()[0]\npercent_greater_than_avg = amzn_df.filter(amzn_df[\"High\"] > avg_high).count() / amzn_df.count() * 100\nprint(f\"Average High: {avg_high}, Percent greater than average: {percent_greater_than_avg:.2f}%\")"}, {"cell_type": "code", "execution_count": 9, "id": "84900316-2afc-45c3-9354-77d2c8e43ca0", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 22:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----+-----------+\n|Year|sum(Volume)|\n+----+-----------+\n|2018|28357952000|\n|2020|24950814000|\n|2016|20775126000|\n|2014|20581334000|\n|2019|19493002000|\n|2015|19142040000|\n|2022|19096256300|\n|2017|17654108000|\n|2021|17076362000|\n|2013|14958114000|\n+----+-----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "#7: What is the total Volume per year? Sort the results based on the total Volume in a decreasing order.\namzn_df.withColumn(\"Year\", year(amzn_df[\"Date\"])).groupBy(\"Year\").sum(\"Volume\").orderBy(\"sum(Volume)\", ascending=False).show()"}, {"cell_type": "code", "execution_count": 10, "id": "ce90b7c0-32b8-4632-aaf2-a914354d2b7d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-----------------+\n|Month|   avg(Adj Close)|\n+-----+-----------------+\n|    1|67.00627561083743|\n|    2|68.95255491623037|\n|    5|69.37138398578199|\n|    3|69.88013780275226|\n|    4|71.78249272596155|\n|    6|73.02435191079809|\n|   12|75.53066592417065|\n|   10|76.20908837556561|\n|   11|76.53358077450977|\n|    7|77.88898079245286|\n|    9|78.86672661951219|\n|    8| 79.5366561040724|\n+-----+-----------------+\n\n"}], "source": "#8: What is the average Adjusted Close price for each month? Sort the results in an increasing order based on the average Adjusted Close price.\namzn_df.withColumn(\"Month\", month(amzn_df[\"Date\"])).groupBy(\"Month\").agg({\"Adj Close\": \"avg\"}).orderBy(\"avg(Adj Close)\").show()"}, {"cell_type": "code", "execution_count": 11, "id": "1e2dd1da-0db5-4dd8-8547-b0fd35ec7238", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "# records with close price < open price: 1272\n"}], "source": "#9: Using SQL commands, create a new data frame that includes the records for the days where closing price is lower than the opening price. \n#   Display the number of records in this new data frame.\namzn_df.createOrReplaceTempView(\"stocks\")\nlow_close_amzn_df = spark.sql(\"SELECT * FROM stocks WHERE `Close` < `Open`\")\nprint(f\"# records with close price < open price: {low_close_amzn_df.count()}\")"}, {"cell_type": "code", "execution_count": 12, "id": "b786bdab-9a91-4bb6-afcc-7a50096265f0", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+-----+----+\n|Year|Month|Days|\n+----+-----+----+\n|2020|    7|   2|\n|2020|    9|   4|\n|2020|   10|   1|\n|2021|    1|   1|\n|2021|    2|   1|\n|2021|   11|   1|\n|2021|   12|   1|\n|2022|    1|   3|\n|2022|    2|   1|\n|2022|    3|   2|\n|2022|    4|   3|\n|2022|    5|   1|\n|2022|    6|   1|\n|2022|    8|   1|\n|2022|   10|   1|\n|2022|   11|   2|\n+----+-----+----+\n\n"}], "source": "#10: Perform the following using SQL commands: \n#    -Count the number of days in each month of each year during the last three years, \n#      where closing price is at least $5 less than the opening price. \n#    -The resulting data frame should be sorted based on Year first and then Month in increasing order.\nrecent_years_amzn_df = spark.sql(\"\"\"\n    SELECT Year, Month, COUNT(*) AS Days\n    FROM (\n        SELECT year(Date) AS Year, month(Date) AS Month\n        FROM stocks\n        WHERE `Close` <= `Open` - 5 AND year(Date) >= 2020\n    )\n    GROUP BY Year, Month\n    ORDER BY Year, Month\n\"\"\")\nrecent_years_amzn_df.show()"}, {"cell_type": "code", "execution_count": null, "id": "bee6b088-eee0-40d9-9bc3-eee1ed13b89d", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}